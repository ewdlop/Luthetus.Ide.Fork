.TH "Source/Tests/Common/Basis Directory Reference" 3 "Version 1.0.0" "Luthetus.Ide" \" -*- nroff -*-
.ad l
.nh
.SH NAME
Source/Tests/Common/Basis Directory Reference
.SH SYNOPSIS
.br
.PP
.SS "Directories"

.in +1c
.ti -1c
.RI "directory \fBBackgroundTasks\fP"
.br
.ti -1c
.RI "directory \fBClipboards\fP"
.br
.ti -1c
.RI "directory \fBCommands\fP"
.br
.ti -1c
.RI "directory \fBComponentRenderers\fP"
.br
.ti -1c
.RI "directory \fBContexts\fP"
.br
.ti -1c
.RI "directory \fBDialogs\fP"
.br
.ti -1c
.RI "directory \fBDimensions\fP"
.br
.ti -1c
.RI "directory \fBDrags\fP"
.br
.ti -1c
.RI "directory \fBDropdowns\fP"
.br
.ti -1c
.RI "directory \fBFileSystems\fP"
.br
.ti -1c
.RI "directory \fBInstallations\fP"
.br
.ti -1c
.RI "directory \fBKeyboards\fP"
.br
.ti -1c
.RI "directory \fBKeymaps\fP"
.br
.ti -1c
.RI "directory \fBKeys\fP"
.br
.ti -1c
.RI "directory \fBMenus\fP"
.br
.ti -1c
.RI "directory \fBMisc\fP"
.br
.ti -1c
.RI "directory \fBNamespaces\fP"
.br
.ti -1c
.RI "directory \fBNotifications\fP"
.br
.ti -1c
.RI "directory \fBOptions\fP"
.br
.ti -1c
.RI "directory \fBPanels\fP"
.br
.ti -1c
.RI "directory \fBReactives\fP"
.br
.ti -1c
.RI "directory \fBReflectives\fP"
.br
.ti -1c
.RI "directory \fBResizes\fP"
.br
.ti -1c
.RI "directory \fBStorages\fP"
.br
.ti -1c
.RI "directory \fBThemes\fP"
.br
.ti -1c
.RI "directory \fBTreeViews\fP"
.br
.ti -1c
.RI "directory \fBWatchWindows\fP"
.br
.in -1c
.SH "Detailed Description"
.PP 
One might decide to have a unit test for every public API\&.

.PP
That is the idea of this folder\&. A one to one unit test per public API at minimum\&.

.PP
Whether every public API will be done or not is being decided upon\&. But, perhaps that wording illustrates the purpose of this folder\&.

.PP
.PP

.PP
Ideas:

.PP
.IP "\(bu" 2
Automate test heatmap\&.\&.\&. If a C# compiler service could identify all publically scoped API, it could verify if after the basis was ran whether all public API was used\&.
.IP "\(bu" 2
Visualize the "basis"\&.\&.\&. so one could look at a given release of \fBLuthetus\&.Ide\fP and then bring up the "UnitTests-Basis-Visualization" for that given release\&.
.IP "  \(bu" 4
Draw a graph of all public API references\&.
.IP "  \(bu" 4
Color the connection green if no exception occurred\&.
.IP "  \(bu" 4
Color the connection red if an exception occurred\&.
.PP

.IP "\(bu" 2
Automate test generation\&.\&.\&.
.IP "  \(bu" 4
Try any meaningful arguments, automatically as their own test cast\&. Ex: a method which accepts an 'int'\&. One could decide that meaningful arguments are [ Int\&.Min, -2, -1, 0, 1, 2 Int\&.Max ]
.IP "  \(bu" 4
Concerns: could automated test cases result in bad outcomes? I\&.e\&. somehow the test case does bank\&.Withdraw(Int\&.Max)\&. As per the concern, none of this automation exists at the moment\&. 
.PP

.PP

